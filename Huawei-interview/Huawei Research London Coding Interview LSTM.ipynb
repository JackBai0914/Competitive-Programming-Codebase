{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Test \n",
    "\n",
    "You will be assesed overall on;\n",
    "\n",
    "1) How far you get in the alloted time.\n",
    "2) Code optimisations.\n",
    "3) Code reusability.\n",
    "4) Code readability.\n",
    "\n",
    "Some hints; \n",
    "\n",
    "1) Take regulaer berak (at least 5 minutes every hour) or changes in activity\n",
    "2) Avoiding awkward, static postures by regularly changing position\n",
    "3) Getting up and moving or doing stretching exercises\n",
    "4) Avoiding eye fatigue by changing focus or blinking from time to time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement a vanilla PPO learning agent and train it on 'acrobot-v1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00005\n",
    "gamma = 0.98\n",
    "lmbda = 0.95\n",
    "\n",
    "#extra-hyperparameter\n",
    "eps_clip = 0.1\n",
    "K_epoch = 3\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.prep_data = []\n",
    "        self.function1   = nn.Linear(6,256)\n",
    "        self.function_pi = nn.Linear(256,3)\n",
    "        self.function_v  = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.function1(x))\n",
    "        x = self.function_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.function1(x))\n",
    "        v = self.function_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "        for transition in self.prep_data:\n",
    "            s, a, r, s_prime, prob_a, done = transition\n",
    "            \n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            prob_a_lst.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_lst.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
    "        self.prep_data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "        \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('Acrobot-v1')\n",
    "    model = PPO()\n",
    "    score = 0.0\n",
    "    print_interval = 200\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        test_a = 0\n",
    "        mn_a = 1000\n",
    "        \n",
    "        while not done:\n",
    "            for t in range(20):\n",
    "                prob = model.pi(torch.from_numpy(s).float())\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "#                 env.render()\n",
    "                s_prime, r, done, info = env.step(a)\n",
    "                test_a = max(test_a, a)\n",
    "                mn_a = min(mn_a, a)\n",
    "\n",
    "                model.prep_data.append((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
    "                s = s_prime\n",
    "\n",
    "                score += r\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# of episode :200, avg score : -199.3\n",
    "# of episode :400, avg score : -116.1\n",
    "# of episode :600, avg score : -105.3\n",
    "# of episode :800, avg score : -97.8\n",
    "# of episode :1000, avg score : -94.1\n",
    "# of episode :1200, avg score : -87.1\n",
    "# of episode :1400, avg score : -86.5\n",
    "# of episode :1600, avg score : -92.0\n",
    "# of episode :1800, avg score : -84.6\n",
    "# of episode :2000, avg score : -90.3\n",
    "# of episode :2200, avg score : -86.3\n",
    "# of episode :2400, avg score : -86.3\n",
    "# of episode :2600, avg score : -84.7\n",
    "# of episode :2800, avg score : -85.3\n",
    "# of episode :3000, avg score : -82.8\n",
    "# of episode :3200, avg score : -87.0\n",
    "# of episode :3400, avg score : -83.5\n",
    "# of episode :3600, avg score : -83.4\n",
    "# of episode :3800, avg score : -84.6\n",
    "# of episode :4000, avg score : -86.2\n",
    "# of episode :4200, avg score : -82.5\n",
    "# of episode :4400, avg score : -86.1\n",
    "# of episode :4600, avg score : -88.2\n",
    "# of episode :4800, avg score : -84.3\n",
    "# of episode :5000, avg score : -87.2\n",
    "# of episode :5200, avg score : -86.0\n",
    "# of episode :5400, avg score : -83.9\n",
    "# of episode :5600, avg score : -89.1\n",
    "# of episode :5800, avg score : -87.8\n",
    "# of episode :6000, avg score : -85.3\n",
    "# of episode :6200, avg score : -86.5\n",
    "# of episode :6400, avg score : -92.4\n",
    "# of episode :6600, avg score : -87.3\n",
    "# of episode :6800, avg score : -86.4\n",
    "# of episode :7000, avg score : -88.6\n",
    "# of episode :7200, avg score : -86.8\n",
    "# of episode :7400, avg score : -86.2\n",
    "# of episode :7600, avg score : -84.5\n",
    "# of episode :7800, avg score : -85.4\n",
    "# of episode :8000, avg score : -88.0\n",
    "# of episode :8200, avg score : -87.8\n",
    "# of episode :8400, avg score : -81.6\n",
    "# of episode :8600, avg score : -88.1\n",
    "# of episode :8800, avg score : -85.2\n",
    "# of episode :9000, avg score : -86.4\n",
    "# of episode :9200, avg score : -83.7\n",
    "# of episode :9400, avg score : -83.5\n",
    "# of episode :9600, avg score : -87.6\n",
    "# of episode :9800, avg score : -85.3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Transition Model: LSTM with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section you will (i) make a dataset of randomly collected rollouts from acrobot-v1 and then (ii) fit an LSTM with attention to this. Your input to the model will be [state,action] and output [next_state]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Model Based PPO: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section you turn your transition model into a gym enviroment and then attempt to train PPO inside of this model.  Compare PPO on the model versus PPO on the real acrobot as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
